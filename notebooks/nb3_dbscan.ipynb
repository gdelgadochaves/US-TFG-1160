{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este cuaderno repetimos los pasos del fichero *nb2_kmeans.ipynb*, pero empleando el algoritmo **DBSCAN** de clustering, cuya fortaleza reside en asignar *clusters* en problemas donde los datos están congregados en grupos mayores, permitiendo el uso de un proceso de inferencia distinto a un algoritmo genérico como k-means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 1: [DBSCAN mediante GPU](https://docs.rapids.ai/api/cuml/stable/api.html#dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y tratamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaciones\n",
    "* **os** y **os.path**: importan utilidades de Python para tratamiento de ficheros y comprobación de rutas.\n",
    "* **cudf**, **cupy** y **cuml**: librerías de RAPIDS.\n",
    "* **pandas** y **numpy**: librerías de manejo de DataFrames y gestión numérica, de vectores y tablas. Equivalentes a cudf y cupy respectivamente para CPU.\n",
    "* **sklearn**: librería de manejo de modelos machine learning, equivalente de cuml en CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuml\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ficheros de utilidades:\n",
    "* **f_northing**: conversión de coordenadas latitud-longitud a norte-este, empleados en manejo de mapas y coordenadas.\n",
    "* **f_northing_numpy**: equivalente de f_northing para estructuras de numpy.\n",
    "* **f_static_data**: funciones para cargar listas de datos estáticos, tales como nombres de ciudades a utilizar desde el directorio /data, columnas a leer por cada fichero CSV, y columnas a usar en el entrenamiento del modelo.\n",
    "* **f_utils**: utilidades de conversión de datos a tipos float32/float64, limpiado de strings de precios, factorización de columnas complejas en mapas numéricos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../utils/f_northing.py\n",
    "%run ../utils/f_northing_numpy.py\n",
    "%run ../utils/f_static_data.py\n",
    "%run ../utils/f_utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de datos:\n",
    "* **cities_to_use**: lista de directorios dentro de /data sobre los que leer datasets en formato CSV. Cada directorio es una ciudad, área, estado o país.\n",
    "* **columns_to_use**: lista de columnas a leer dentro de cada CSV. Todas las columnas deben existir y sus nombres deben coincidir.\n",
    "* **columns_to_fit**: lista de columnas a utilizar para el entrenamiento de modelos de machine learning. Nos permite hacer múltiples usos de un dataset sin necesidad de leerlo de nuevo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cities_to_use = ['sevilla']\n",
    "#cities_to_use = ['shanghai']\n",
    "#cities_to_use = cities_to_use_1()\n",
    "cities_to_use = cities_to_use_2()\n",
    "\n",
    "columns_to_use = ['host_id', 'host_response_rate', 'host_acceptance_rate',\n",
    "                  'latitude', 'longitude', 'accommodates', 'price', 'number_of_reviews', 'reviews_per_month',\n",
    "                 'neighbourhood_cleansed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El DataFrame **listings** representa el conjunto de datos final sobre el que realizar operaciones de ciencia de datos o machine learning. Por cada directorio de datos, leemos todos los ficheros CSV disponibles y los agregamos a **listings**.\n",
    "* **standard_object_type**: algunas columnas no tienen un formato único de datos; esto puede deberse a diferentes departamentos o metodologías empleadas durante los varios *scraping* de datos de la fuente. Esta función convierte columnas (detectadas manualmente a priori) en tipo genérico *object*, que después convertiremos a un tipo de datos más apropiado para nuestro uso.\n",
    "* **drop_duplicates**: dado que utilizamos múltiples *datasets* por ciudades, hay listados de AirBnB que no cambian durante varios meses. En este caso, no necesitamos datos redundantes, y los removemos del DataFrame final.\n",
    "* **reset_index**: los datos de cada fichero se han unido mediante un DataFrame diferente, y por lo tanto los índices del DataFrame quedan mezclados al final del proceso. Con esta instrucción forzamos el índice de **listings** a ordenarse. La instrución añade una columna adicional con el índice antiguo, que podemos obviar con el parámetro drop=**True**.\n",
    "* **shape**: instrucción que devuelve el tamaño completo del DataFrame en formato (filas, columnas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = cudf.DataFrame()\n",
    "\n",
    "for city in cities_to_use:\n",
    "    directory = '../data/' + city + '/'\n",
    "    if os.path.exists(directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith('.csv'):\n",
    "                temp_df = cudf.read_csv(directory + file, usecols = columns_to_use)\n",
    "                standard_object_type(temp_df, ['host_acceptance_rate', 'neighbourhood_cleansed'])\n",
    "                if listings.size == 0:\n",
    "                    listings = temp_df\n",
    "                else:\n",
    "                    for column in listings.columns:\n",
    "                        if listings[column].dtype != temp_df[column].dtype:\n",
    "                            print('Found error: '+column+' type '+listings[column].dtype.name+' doesnt match '+temp_df[column].dtype.name)\n",
    "                    listings = listings.append(temp_df)\n",
    "                    \n",
    "listings = listings.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento simple:\n",
    "* **type_conversion**: convertimos todas las columnas a un tipo de datos común: *float32* y *float64* son los más aceptados por los algoritmos de RAPIDS.\n",
    "* **column_factorize**: dado que algunos algoritmos no suportan datos no numéricos, factorizamos las columnas de texto. La operación factorize() convierte una columna con valores no numéricos a un mapa en el que cada valor único se representa por un integer. En nuestro caso, no necesitaremos el mapa que reconoce cada valor, pero es posible usarlo para devolver formato a los datos de cara a estudio o representación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_conversion(listings, ['host_id', 'accommodates', 'number_of_reviews', 'reviews_per_month'])\n",
    "column_factorize(listings, ['neighbourhood_cleansed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamiento de cadenas:\n",
    "* **clean_format_strings**: eliminamos el carácter '%' de campos que lo contienen, y convertimos el valor resultante a *float32*.\n",
    "* **clean_format_price**: similar a la función anterior, pero eliminando caracteres de moneda (en nuestro caso, '$' y la coma de cantidades numéricas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_format_strings(listings, ['host_response_rate', 'host_acceptance_rate'])\n",
    "clean_format_price(listings, ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fin de visualizar los datos en un formato 2D similar a un mapa, convertimos las coordatas latitud-longitud en distancias norte-este, y les asignamos columnas nuevas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_lat = cp.asarray(listings['latitude'])\n",
    "cupy_long = cp.asarray(listings['longitude'])\n",
    "n_cupy_array, e_cupy_array = latlong2osgbgrid_cupy(cupy_lat, cupy_long)\n",
    "listings['northing'] = cudf.Series(n_cupy_array).astype('float32')\n",
    "listings['easting'] = cudf.Series(e_cupy_array).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualización de las primeras 5 filas, para verificar el tratamiento de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de un algoritmo DBSCAN para visualizar clusters de listados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN funciona de manera similar a otros algoritmos de clustering, aunque a nivel del código de RAPIDS existen algunas diferencias.\n",
    "\n",
    "Hemos seleccionado un subconjunto de listados dentro del *dataset* completo, aquellos con un precio igual o superior a $200, a fin de visualizar *clusters* distintivos.\n",
    "\n",
    "Parámetros:\n",
    "* **eps**: máxima distancia entre puntos permitida por *cluster*. En nuestro problema las coordenadas están expresadas en metros, siendo este valor una distancia expresada en dicha unidad.\n",
    "\n",
    "Funciones:\n",
    "* **fit_predict**: ejecuta DBSCAN sobre el modelo y calcula automáticamente los centros de los *clusters*.\n",
    "* **nunique**: función de cuDF que nos permite ver el número total de *clusters*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dbscan = cuml.DBSCAN(eps=150)\n",
    "price_df = listings[listings['price'] >= 200].reset_index(drop=True)\n",
    "price_df['cluster'] = dbscan.fit_predict(price_df.loc[:, ['northing', 'easting']])\n",
    "price_df['cluster'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 2: DBSCAN mediante CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y tratamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lectura y tratamiento de los datos iniciales sigue el mismo proceso que para GPU:\n",
    "1. Creamos un nuevo DataFrame vacío.\n",
    "2. Recorremos los directorios disponibles.\n",
    "3. Agregamos los CSV, convirtiendo columnas dispares a tipo *object* si fuera necesario.\n",
    "4. Eliminamos duplicados.\n",
    "5. Reordenamos el índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_cpu = pd.DataFrame()\n",
    "\n",
    "for city in cities_to_use:\n",
    "    directory = '../data/' + city + '/'\n",
    "    if os.path.exists(directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith('.csv'):\n",
    "                temp_df_cpu = pd.read_csv(directory + file, usecols = columns_to_use)\n",
    "                standard_object_type(temp_df_cpu, ['host_acceptance_rate', 'neighbourhood_cleansed'])\n",
    "                if listings_cpu.size == 0:\n",
    "                    listings_cpu = temp_df_cpu\n",
    "                else:\n",
    "                    for column in listings_cpu.columns:\n",
    "                        if listings_cpu[column].dtype != temp_df_cpu[column].dtype:\n",
    "                            print('Found error: '+column+' type '+listings_cpu[column].dtype.name+' doesnt match '+temp_df_cpu[column].dtype.name)\n",
    "                    listings_cpu = listings_cpu.append(temp_df_cpu)\n",
    "                    \n",
    "listings_cpu = listings_cpu.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las funciones **type_conversion**, **column_factorize** y **clean_format_strings** son las mismas empleadas para GPU, ya que las operaciones subyacentes tienen la misma semántica y uso para cuDF/cupy como para pandas/numpy.\n",
    "\n",
    "* **clean_format_price_cpu**: versión específica del tratado de cadenas de precios para CPU. Es necesario cambiar ligeramente el órden y número de operaciones para que numpy pueda ejecutar el mismo reemplazo de cadenas de precios que en GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_conversion(listings_cpu, ['host_id', 'accommodates', 'number_of_reviews', 'reviews_per_month'])\n",
    "column_factorize(listings_cpu, ['neighbourhood_cleansed'])\n",
    "\n",
    "clean_format_strings(listings_cpu, ['host_response_rate', 'host_acceptance_rate'])\n",
    "clean_format_price_cpu(listings_cpu, ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única diferencia en la conversión de coordenadas latitud-logitud a norte-este es que utilizamos funciones aritméticas provistas por numpy en lugar de cupy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy_lat = listings_cpu['latitude'].to_numpy()\n",
    "numpy_long = listings_cpu['longitude'].to_numpy()\n",
    "n_numpy_array, e_numpy_array = latlong2osgbgrid_numpy(numpy_lat, numpy_long)\n",
    "listings_cpu['northing'] = pd.Series(n_numpy_array).astype('float32')\n",
    "listings_cpu['easting'] = pd.Series(e_numpy_array).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_cpu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz de scikit-learn para DBSCAN es muy similar a la de RAPIDS, con la diferencia de que el resultado de **fit_predict** debe ser convertido a un array de pandas antes de introducirlo en el DataFrame final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dbscan_cpu = DBSCAN(eps=150)\n",
    "price_df_cpu = listings_cpu[listings_cpu['price'] >= 200].reset_index(drop=True)\n",
    "price_cpu_cluster = dbscan_cpu.fit_predict(price_df_cpu.loc[:, ('northing', 'easting')])\n",
    "price_df_cpu['cluster'] = pd.array(price_cpu_cluster)\n",
    "price_df_cpu['cluster'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Nota:*** no llamar a la instrucción reset si se desea visualizar los datos (ver Sección 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 3: Visualización de resultados mediante cuXfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se desea, es posible visualizar los *clusters* en un mapa, similar al del fichero *nb1_visual.ipynb*, con un selector por *cluster*."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import cuxfilter as cxf\n",
    "\n",
    "neighborhood_map = dict(zip(range(len(neighborhood_names)), neighborhood_names.values_host))\n",
    "cxf_data = cxf.DataFrame.from_dataframe(price_df)\n",
    "\n",
    "chart_width = 600\n",
    "scatter_chart = cxf.charts.datashader.scatter(x='easting', y='northing', \n",
    "                                              width=chart_width, \n",
    "                                              height=int((listings['easting'].max() - listings['easting'].min()) / \n",
    "                                                         (listings['northing'].max() - listings['northing'].min()) *\n",
    "                                                          chart_width))\n",
    "\n",
    "widget = cxf.charts.panel_widgets.multi_select('cluster')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dashboard = cxf_data.dashboard([scatter_chart, widget], theme=cxf.themes.dark, data_size_widget=True)\n",
    "dashboard.show('http://localhost', port=8789)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dashboard.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
