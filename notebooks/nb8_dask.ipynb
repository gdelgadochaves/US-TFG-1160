{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Dask](https://dask.org/) es una librería de Python que permite paralelizar procesos en múltiples CPU's, principalmente el conjunto pandas/numpy/scikit-learn, mediante estructuras de datos compatibles (tales como el DataFrame de Dask). A fin de ser utilizable por RAPIDS, se ha creado una librería de conexión llamada dask-cudf.\n",
    "\n",
    "Dask permite crear entornos con múltiples instancias asignables a GPU's separadas, de tal modo que ciertas operaciones se añaden a un pipeline de manera asíncrona, y son ejecutadas en paralelo una vez definido el proceso completo. Esto nos permite hacer uso de múltiples GPU's para tratar datos y ejectutar algoritmos de machine learning mediante dask-cuml.\n",
    "\n",
    "Aunque es posible crear todo un proceso de lectura y transformación de datos en paralelo, el objetivo de este cuaderno es medir los tiempos de ejecución de los algoritmos disponibles en dask-cuml que ya se han probado en los cuadernos anteriores. Por ello, la única sección de este cuaderno se ejecuta en GPU, y tan sólo el entrenamiento de modelos y cálculo de soluciones o predicciones es ejecutado mediante Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** este cuaderno emplea técnicas descritas en los cuadernos nb1 a nb6. Se recomienda haber leído y ejecutado al menos algunos de estos cuadernos, a fin de comprender los procesos ejecutados a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sección 1: computación de modelos machine learning en múltiples GPU's en paralelo mediante Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y tratamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaciones y carga de scripts y datos inicial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "import cudf\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml.metrics.regression import mean_squared_error as mnsq\n",
    "\n",
    "%run ../utils/f_northing.py\n",
    "%run ../utils/f_northing_numpy.py\n",
    "%run ../utils/f_price_range.py\n",
    "%run ../utils/f_static_data.py\n",
    "%run ../utils/f_utils.py\n",
    "\n",
    "cities_to_use = ['sevilla']\n",
    "#cities_to_use = ['shanghai']\n",
    "#cities_to_use = cities_to_use_1()\n",
    "#cities_to_use = cities_to_use_2()\n",
    "\n",
    "columns_to_use = columns_to_use()\n",
    "columns_to_fit = columns_to_fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaciones únicas de este cuaderno:\n",
    "* **subprocess:** librería de Python que nos permite crear y gestionar subprocesos. Se emplea para determinar la dirección local en la que correremos el cluster de Dask.\n",
    "* **dask, dask_cuda, dask_cudf:** librerías de Dask para ejecución mediante GPU (CUDA y cuDF).\n",
    "* **cuml.dask:** sub-librería gestionada por RAPIDS para la ejecución de modelos cuml en instancias multi GPU con Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import dask\n",
    "\n",
    "from dask.distributed import Client, wait, progress\n",
    "from dask_cuda import LocalCUDACluster\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "\n",
    "from dask import compute\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import dask_cudf\n",
    "from cuml.dask.cluster import KMeans as DKMeans\n",
    "from cuml.dask.cluster import DBSCAN as DDBSCAN\n",
    "from cuml.dask.neighbors import NearestNeighbors as DNN\n",
    "from cuml.dask.linear_model import LinearRegression as DLinReg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediante Dask y la librería de subprocesos de Python, creamos un subproceso albergando un cluster de CUDA, y un cliente ejecutable sobre el que cargaremos los procesos de Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"hostname --all-ip-addresses\"\n",
    "process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()\n",
    "IPADDR = str(output.decode()).split()[0]\n",
    "\n",
    "cluster = LocalCUDACluster(ip=IPADDR)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** si se desea ejecutar regresión linear, ignorar la siguiente celda y ejecutar la carga y tratamiento de datos de la sección **'Tratamiento especial de datos para regresión linear'**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carga y tratamiento de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "listings = cudf.DataFrame()\n",
    "\n",
    "for city in cities_to_use:\n",
    "    directory = '../data/' + city + '/'\n",
    "    if os.path.exists(directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith('.csv'):\n",
    "                temp_df = cudf.read_csv(directory + file, usecols = columns_to_use)\n",
    "                standard_object_type(temp_df, ['host_acceptance_rate', 'neighbourhood_cleansed'])\n",
    "                if listings.size == 0:\n",
    "                    listings = temp_df\n",
    "                else:\n",
    "                    for column in listings.columns:\n",
    "                        if listings[column].dtype != temp_df[column].dtype:\n",
    "                            print('Found error: '+column+' type '+listings[column].dtype.name+' doesnt match '+temp_df[column].dtype.name)\n",
    "                    listings = listings.append(temp_df)\n",
    "                    \n",
    "listings = listings.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "type_conversion(listings, ['host_id', 'accommodates', 'number_of_reviews', 'reviews_per_month'])\n",
    "column_factorize(listings, ['neighbourhood_cleansed'])\n",
    "\n",
    "clean_format_strings(listings, ['host_response_rate', 'host_acceptance_rate'])\n",
    "clean_format_price(listings, ['price'])\n",
    "\n",
    "cupy_lat = cp.asarray(listings['latitude'])\n",
    "cupy_long = cp.asarray(listings['longitude'])\n",
    "n_cupy_array, e_cupy_array = latlong2osgbgrid_cupy(cupy_lat, cupy_long)\n",
    "listings['northing'] = cudf.Series(n_cupy_array).astype('float32')\n",
    "listings['easting'] = cudf.Series(e_cupy_array).astype('float32')\n",
    "\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicación de algoritmos mediante cuml.dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Nota:* se recomienda no ejecutar todas las celdas a continuación en una sóla ejecución, a menos que se disponga de suficiente memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos previamente tratados están actualmente gestionados en memoria de GPU mediante cuDF. Para poder emplearlos en el cluster de Dask, debemos convertirlos a un DataFrame de Dask. La operación persist() guarda estos cambios en la memoria asignada al cluster, de modo que no es necesario volver a convertir los datos para ejecutar múltiples pruebas en este cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_dask = dask.dataframe.from_pandas(listings, npartitions=4)\n",
    "listings_dask.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "km = DKMeans(n_clusters=5)\n",
    "km.fit(listings_dask[['easting', 'northing']])\n",
    "listings['kmeans'] = km.labels_\n",
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La interfaz de DBSCAN en cuml.dask no acepta el objeto DataFrame de Dask, sino el DataFrame de cuDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dbscan = DDBSCAN(client=client, eps=150)\n",
    "price_df = listings[listings['price'] >= 200].reset_index(drop=True)\n",
    "price_df['cluster'] = dbscan.fit_predict(price_df.loc[:, ['northing', 'easting']])\n",
    "price_df['cluster'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knn = DNN(n_neighbors=3)\n",
    "listings_cheap = listings[listings['price'] <= 50.0].reset_index(drop=True)\n",
    "listings_expensive = listings[listings['price'] >= 100.0].reset_index(drop=True)\n",
    "listings_cheap_cols = listings_cheap[['easting', 'northing']]\n",
    "listings_expensive_cols = listings_expensive[['easting', 'northing']]\n",
    "\n",
    "dask_cheap = dask.dataframe.from_pandas(listings_cheap_cols, npartitions=4)\n",
    "dask_expensive = dask.dataframe.from_pandas(listings_expensive_cols, npartitions=4)\n",
    "\n",
    "knn.fit(dask_cheap)\n",
    "dask_distances, dask_indices = knn.kneighbors(dask_expensive)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "distances = dask_distances.compute()\n",
    "distances"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "indices = dask_indices.compute()\n",
    "indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamiento especial de datos para regresión linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = cudf.DataFrame()\n",
    "\n",
    "for city in cities_to_use:\n",
    "    directory = '../data/' + city + '/'\n",
    "    if os.path.exists(directory):\n",
    "        for file in os.listdir(directory):\n",
    "            if file.endswith('.csv'):\n",
    "                temp_df = cudf.read_csv(directory + file, usecols = columns_to_use)\n",
    "                standard_object_type(temp_df, ['host_acceptance_rate', 'neighbourhood_cleansed'])\n",
    "                if(temp_df['host_total_listings_count'].dtype != 'float64'):\n",
    "                    temp_df['host_total_listings_count'] = temp_df['host_total_listings_count'].fillna(-1).astype('float64')\n",
    "                if(temp_df['bathrooms'].dtype != 'float64'):\n",
    "                    temp_df['bathrooms'] = temp_df['bathrooms'].fillna(-1).astype('float64')\n",
    "                if(temp_df['bedrooms'].dtype != 'float64'):\n",
    "                    temp_df['bedrooms'] = temp_df['bedrooms'].fillna(-1).astype('float64')\n",
    "                if(temp_df['beds'].dtype != 'float64'):\n",
    "                    temp_df['beds'] = temp_df['beds'].fillna(-1).astype('float64')\n",
    "                if listings.size == 0:\n",
    "                    listings = temp_df\n",
    "                else:\n",
    "                    for column in listings.columns:\n",
    "                        if listings[column].dtype != temp_df[column].dtype:\n",
    "                            print('Found error: '+column+' type '+listings[column].dtype.name+' doesnt match '+temp_df[column].dtype.name)\n",
    "                    listings = listings.append(temp_df)\n",
    "                    \n",
    "listings = listings.drop_duplicates()\n",
    "listings = listings.reset_index(drop=True)\n",
    "\n",
    "type_conversion_64(listings, ['host_id', 'accommodates', 'number_of_reviews', 'reviews_per_month', 'minimum_nights', 'maximum_nights', 'availability_30', 'availability_90', 'availability_365', 'number_of_reviews_ltm', 'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value', 'host_total_listings_count', 'bathrooms', 'bedrooms', 'beds'])\n",
    "column_factorize_64(listings, ['neighbourhood_cleansed', 'host_response_time', 'host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'property_type', 'room_type', 'instant_bookable'])\n",
    "\n",
    "clean_format_strings_64(listings, ['host_response_rate', 'host_acceptance_rate'])\n",
    "clean_format_price_64(listings, ['price'])\n",
    "listings['price'] = listings['price'].applymap(priceRange, 'float64')\n",
    "\n",
    "cupy_lat = cp.asarray(listings['latitude'])\n",
    "cupy_long = cp.asarray(listings['longitude'])\n",
    "n_cupy_array, e_cupy_array = latlong2osgbgrid_cupy(cupy_lat, cupy_long)\n",
    "listings['northing'] = cudf.Series(n_cupy_array).astype('float64')\n",
    "listings['easting'] = cudf.Series(e_cupy_array).astype('float64')\n",
    "\n",
    "listings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicación de regresión linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "regression = DLinReg()\n",
    "x_train, x_test, y_train, y_test = cuml.train_test_split(listings[columns_to_fit], listings['price'], train_size=0.9)\n",
    "x_test_index = x_test.reset_index(drop=True)\n",
    "y_test_index = y_test.reset_index(drop=True)\n",
    "\n",
    "dask_x_train = dask.dataframe.from_pandas(x_train, npartitions=4)\n",
    "dask_y_train = dask.dataframe.from_pandas(y_train, npartitions=4)\n",
    "dask_x_test = dask.dataframe.from_pandas(x_test_index, npartitions=4)\n",
    "dask_y_test = dask.dataframe.from_pandas(y_test_index, npartitions=4)\n",
    "\n",
    "regression.fit(dask_x_train, dask_y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predictions = regression.predict(dask_x_test)\n",
    "y_results = predictions.compute()\n",
    "print(y_results[0:10])\n",
    "print(y_test_index[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mnsq(y_test_index, y_results)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
